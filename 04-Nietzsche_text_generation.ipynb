{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 04-Nietzsche_text_generation.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [
        {
          "file_id": "https://github.com/karlafej/keras_pyconCZ/blob/master/04-Nietzsche_text_generation.ipynb",
          "timestamp": 1527991206389
        }
      ],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "w_e5GXnWH9tO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Character-Level Text Generation\n",
        "\n",
        "In this lab, you will learn to utilize recurrent neural networks for generating text that resembles work of Friedrich Nietzsche. You will go through data cleaning, search for optimal parameters to minimize loss function and get a high quality Nietzsche-like quote."
      ]
    },
    {
      "metadata": {
        "id": "RX8-d2WTZ7rc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import re, collections            # for text processing\n",
        "from google.colab import files    # for download files"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BLMS2qirKgCs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 1) Download the dataset"
      ]
    },
    {
      "metadata": {
        "id": "Tzfhr5p1aFWD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "path = keras.utils.data_utils.get_file('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "with open(path, encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print('corpus length:', len(text))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iDGmLs5eK3NI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 2) Text preprocessing\n",
        "\n",
        "For simplicity, we convert everything to lower case and remove ends of lines. We might want to remove characters that occur just a few times or look redundant (smaller alphabet = faster calculation)."
      ]
    },
    {
      "metadata": {
        "id": "fbvOJk0JauHR",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "text = text.lower().replace(\"\\n\", \" \")\n",
        "text = re.sub('[ëæéä0-9_\\[\\]=\\(\\)]', '', text)\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "num_chars = len(chars)\n",
        "print('total characters in vocabulary:', num_chars)\n",
        "\n",
        "charcounts = collections.Counter(list(text))\n",
        "sorted(charcounts.items(), key=lambda i: i[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aOT2fC_-MJZD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 3) Cut the text in semi-redundant sequences\n",
        "\n",
        "For training, the test is cut into smaller pieces of the same length. Longer pieces enable better context but needs more time and memory for training."
      ]
    },
    {
      "metadata": {
        "id": "su1RvGsRbLxc",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "SEQ_LENGTH = 40   # length of sequences\n",
        "STEP = 10         # shift in cursor between sequences\n",
        "DEPTH = 1         # number of hidden LSTM/GRU layers\n",
        "UNIT_SIZE = 128   # number of units per LSTM\n",
        "DROPOUT = 0.1     # dropout parameter"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OBXopIx-nwU8",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sentences = list()\n",
        "targets = list()\n",
        "for i in range(0, len(text) - SEQ_LENGTH - 1, STEP):\n",
        "    sentences.append(text[i: i + SEQ_LENGTH])\n",
        "    targets.append(text[i + 1: i + SEQ_LENGTH + 1])\n",
        "print('number of sequences:', len(sentences))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qcxSCl-0NUHF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 4) Vectorization\n",
        "\n",
        "One reason to do this is that entering raw numbers into a RNN may not make sense\n",
        "    because it assumes an ordering for catergorical variables."
      ]
    },
    {
      "metadata": {
        "id": "aYavT4Lko2t-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# dictionaries to convert characters to numbers and vice-versa\n",
        "char_to_indices = dict((c, i) for i, c in enumerate(chars))\n",
        "indices_to_char = dict((i, c) for i, c in enumerate(chars))\n",
        "\n",
        "X = np.zeros((len(sentences), SEQ_LENGTH, num_chars), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), SEQ_LENGTH, num_chars), dtype=np.bool)\n",
        "for i in range(len(sentences)):\n",
        "    sentence = sentences[i]\n",
        "    target = targets[i]\n",
        "    for j in range(SEQ_LENGTH):\n",
        "        X[i][j][char_to_indices[sentence[j]]] = 1\n",
        "        y[i][j][char_to_indices[target[j]]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O_fGbXpYP_3d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 5) Model definition\n",
        "\n",
        "One, two (or three) layers of LSTM and dropout, followed by dense connected layer and softmax. You can experiment and modify this code: use GRU (keras.layers.GRU) instead of LSTM, try SGD or Adam optimizers instead of RMSprop and modify learning rate (lr parameter)."
      ]
    },
    {
      "metadata": {
        "id": "XOHedg-vboEs",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model = keras.models.Sequential()\n",
        "for _ in range(DEPTH):\n",
        "    model.add(keras.layers.LSTM(UNIT_SIZE, input_shape=(None, num_chars), return_sequences=True))\n",
        "    model.add(keras.layers.Dropout(DROPOUT))\n",
        "model.add(keras.layers.wrappers.TimeDistributed(keras.layers.Dense(num_chars)))\n",
        "model.add(keras.layers.Activation('softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lTXKpmFLdpG7",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sS1LR_jlRiEd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Helper functions: Generating text from the model\n",
        "\n",
        "The function **sample** takes the trained model and get you a sample of a text generated from it. \n",
        "You can set the beginning (`set.seed`) and `temperature`. Lower temperature makes text more confident (but also more conservative)."
      ]
    },
    {
      "metadata": {
        "id": "fKDuSeEpd8Fr",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def multinomial_with_temperature(preds, temperature=1.0):\n",
        "    \"\"\"\n",
        "    Helper function to sample from a multinomial distribution (+adj. for temperature)\n",
        "    \"\"\" \n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds + 1e-8) / temperature  \n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)\n",
        "\n",
        "\n",
        "def sample(model, char_to_indices, indices_to_char, \n",
        "           seed_string=\" \", temperature=1.0, test_length=150):\n",
        "    \"\"\"\n",
        "    Generates text of test_length length from model starting with seed_string.\n",
        "    \"\"\"\n",
        "    num_chars = len(char_to_indices.keys())\n",
        "    for i in range(test_length):\n",
        "        test_in = np.zeros((1, len(seed_string), num_chars))\n",
        "        for t, char in enumerate(seed_string):\n",
        "            test_in[0, t, char_to_indices[char]] = 1\n",
        "        entire_prediction = model.predict(test_in, verbose=0)[0]\n",
        "        next_index = multinomial_with_temperature(entire_prediction[-1], temperature)\n",
        "        next_char = indices_to_char[next_index]\n",
        "        seed_string = seed_string + next_char\n",
        "    return seed_string"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B9aqL0FsTr1t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 6) Model training\n",
        "\n",
        "Each time you run the code below, the model is trained for 10 epochs  (each sequence is visited 10 times). If the quality of predictions is not sufficient, you can add another 10 epochs, etc."
      ]
    },
    {
      "metadata": {
        "id": "-GWjeLVYeBz3",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "history = model.fit(X, y,\n",
        "            batch_size=1024,\n",
        "            epochs=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XRPywzlTUyBj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 7) Generate text\n",
        "\n",
        "Is it good? Congratulation! You can save and download the model with the code below. Does it need improvement? Either you need more training (Step 6) or you need to change your parameters or model definition (Steps 3 and 5)."
      ]
    },
    {
      "metadata": {
        "id": "UZnAKchWmigy",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sample(model, char_to_indices=char_to_indices, indices_to_char=indices_to_char, seed_string=\"truth\", temperature=0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zq5jWTRmV0vK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Step 8) Saving the model"
      ]
    },
    {
      "metadata": {
        "id": "hLMURFCF7ncu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "model_filename = 'nietzsche.loss{0:.2f}.h5'.format(history.history['loss'][-1])\n",
        "model.save(model_filename)\n",
        "files.download(model_filename)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jQUKJwW2WEgs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Acknowledgement\n",
        "\n",
        "This notebook was adapted from Michael Zhang's [Char-RNN](https://github.com/michaelrzhang/Char-RNN) and [lstm_text_generation.py](https://github.com/keras-team/keras/blob/master/examples/lstm_text_generation.py) example in keras github repo. Both were inspired from Andrej Karpathy's blog post [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)."
      ]
    },
    {
      "metadata": {
        "id": "FzWOWh-haGfx",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
